---
layout: post
title: "Bow down before the power of MCMC: Markov chain là gì."
author: cuongbui
tags: [math, algorithm, algebra, probability, coding, cryptography]
categories: [security, algorithm]
---

Bài mở đầu của tôi trên nền tảng viết blog mới.
<!--more-->

# Blog cũ, Blog mới.
Hôm nay, tôi rất hân hạnh vì được các bro rủ vào team **AIIR** để làm tác giả đóng góp các bài viết kỹ thuật.
Càng vinh dự hơn khi là người đầu tiên mở màn cho blog, do đó tôi sẽ bắt đầu với một seri bài viết không tầm thường.

Lúc trước, tôi cũng đã từng viết blog cá nhân, cũng lười nên ít khi viết bài mới. Tuy nhiên bây giờ, blog đó sẽ chỉ là nơi 
chứa những bài viết mang tính cá nhân nhiều hơn; còn các bài viết kỹ thuật sẽ được đưa lên blog này; và tôi cũng sẽ không
đưa các bài đã viết ở blog cũ lên đây renew. Các bạn nếu quan tâm có thể tham khảo tại [Ngọc Sơn Bùi Văn Cường](https://nsbvc.blogspot.com/)

# Mở màn về seri bài viết.
Trong cái tên của blog này có liên quan đến **AI**, đây là thứ tôi ko thích lắm. Không phải vì không biết về nó, cũng không 
phải vì hiện nay xã hội gáy quá nhiều về AI mà ko hiểu j, mà là vì các thành viên trong nhóm chúng tôi đều hướng đến trở thành
**[kỹ sư](https://nsbvc.blogspot.com/2019/09/ky-su-la-bac-thay-ky-thuat-ko-phai-la.html)**, chứ không đơn thuần chỉ hiểu 
biết 1 trong 2 thứ là **algorithm** hoặc **engineering**.

Do đó, seri bài viết này sẽ nói về một thứ rất quan trọng, và có ứng dụng trong cả Machine Learning và một chút liên quan 
đến Cryptography (vui thôi). Đó là **MCMC (Markov chain Monte Carlo)**. Những ai học tập hay nghiên cứu về Machine Learning thì
chắc chắn ko thể ko biết MCMC. Tuy nhiên để mà hiểu rõ mọi thứ về MCMC thì ko phải đơn giản; nhất là với một thanh niên kém 
giỏi Toán như tôi thì quá trình tìm hiểu về MCMC lại là 1 khoảng thời gian ko hề ngắn và đơn giản tí nào. Đọc ko biết bao 
nhiêu lần, nhưng cảm giác cứ luôn như này vậy!

![Thinking](/assets/img/posts/cuongbui/suynghi.jpg){: .center_image }

May sao "cần cù bù siêng năng", tôi cũng đã ngộ ra được vài phần về MCMC để tiếp tục có thể nghiên cứu về Machine Learning 
mà không bị thấy khó chịu khi phải cưỡi ngựa xem hoa.

Seri bài viết này tôi đã định viết lên blog cá nhân từ lâu rồi, nhưng cũng vì lười vs bận mà chưa viết được. Hôm nay cũng là 
mở màn luôn cho blog mới này của ae.

# MCMC thật ra là j?
## ```'MCMC' = str('MC' + 'MC')```
Yes, indeed, MCMC bao gồm 2 thành phần là **MC (Markov chain)** và **MC (Monte Carlo)**. Vậy hẳn là kỹ thuật này có liên quan 
đến 2 thứ là **Markov chain** và cái thành phố **Monte Carlo** của Monaco. Đặt tên tài tình thật, theo như Wiki thì Monte Carlo 
là thành phố nổi tiếng với các sòng bài, khá là liên quan đến Xác suất thống kê. Còn Markov chain là một mô hình toán học 
do nhà toán học người Nga tên là Markov tìm ra. Lần đầu tiên chúng tôi được tìm hiểu về nó là trong môn Toán chuyên đề của 
Sư phụ tôi dạy hồi năm 3 đại học. Feeling amazing!

Tóm lại, MCMC là **lớp các thuật toán lấy mẫu** một cách **continuous** các **phân phối xác suất** dựa trên việc xây dựng Markov chain 
(chuỗi Markov) theo từng thời điểm lấy mẫu cũng như kỹ thuật ra quyết định j đó liên quan đến cờ bạc ở thành phố Monte Carlo, 
dựa trên xác suất lựa chọn quyết định có lợi.

## Tại sao lại phải là MCMC?
### Phân phối xác suất.
Mục đích của MCMC là **lấy mẫu các phân phối xác suất**! Lấy mẫu (sampling) một phân phối xác suất là thủ tục tính random 
để trả về giá trị của biến ngẫu nhiên ứng với phân phối xác suất đó. Ví dụ biến ngẫu nhiên thể hiện tỷ lệ xuất hiện mặt ngửa 
trong hành động tung xúc xắc sẽ được biểu diễn bằng phân phối **Bernoully** với công thức $$


You can render LaTeX mathematical expressions using [KaTeX](https://khan.github.io/KaTeX/):

The *Gamma function* satisfying $$\Gamma(n) = (n-1)!\quad\forall n\in\mathbb N$$ is via the Euler integral

$$
\Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt\,.
$$

$$
\begin{align*}
  & \phi(x,y) = \phi \left(\sum_{i=1}^n x_ie_i, \sum_{j=1}^n y_je_j \right)
  = \sum_{i=1}^n \sum_{j=1}^n x_i y_j \phi(e_i, e_j) = \\
  & (x_1, \ldots, x_n) \left( \begin{array}{ccc}
      \phi(e_1, e_1) & \cdots & \phi(e_1, e_n) \\
      \vdots & \ddots & \vdots \\
      \phi(e_n, e_1) & \cdots & \phi(e_n, e_n)
    \end{array} \right)
  \left( \begin{array}{c}
      y_1 \\
      \vdots \\
      y_n
    \end{array} \right)
\end{align*}
$$